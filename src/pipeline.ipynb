{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikeras[tensorflow] --user -q\n",
    "# %pip install PythonMETAR -q\n",
    "# %pip install xgboost -q\n",
    "# %pip install opencv-python -q\n",
    "# %pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from joblib import dump, load\n",
    "from time import time, sleep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow import keras, test, random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from keras.losses import MeanSquaredError\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax, Adadelta, Adagrad, RMSprop\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from PythonMETAR import Metar\n",
    "from xgboost import XGBRegressor\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, \\\n",
    "    ReduceLROnPlateau, TerminateOnNaN, BackupAndRestore\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import requests as req\n",
    "import cv2\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    device_name = test.gpu_device_name()\n",
    "    if(device_name):\n",
    "        print('Found GPU at: {}'.format(device_name))\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "except:\n",
    "    print('GPU device not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values below 2000 records generate inconsistencies in the logic of feature engineer\n",
    "training_sample_size = 1 # 0 a 1 (0% a 100%)\n",
    "fold_cross_validation = 5\n",
    "ann_epochs = 1000\n",
    "pca_n_components = 25\n",
    "pca_n_components_mxgb = 10\n",
    "xgb_estimators = 1000000\n",
    "verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 25\n",
    "\n",
    "#Numpy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# TensorFlow\n",
    "random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_selected_cols = ['origem', 'destino', 'metar', 'hora_ref', 'path', 'snapshot_radar']\n",
    "numerical_selected_cols = ['troca', 'esperas']\n",
    "\n",
    "selected_cols = categorical_selected_cols + numerical_selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsDefault = {\n",
    "    'idate': '2022-06-02', # Data inicial da consulta | Example : AAAA-MM-DD\n",
    "    'fdate': '2023-05-11' # Data final da consulta | Example : AAAA-MM-DD\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(route, paramsWithoutToken = paramsDefault, token = 'a779d04f85c4bf6cfa586d30aaec57c44e9b7173'):\n",
    "    \"\"\"\n",
    "    Fetches data from a specific API, handling exceptions and returning the data as a DataFrame.\n",
    "\n",
    "    :param route: String representing the API endpoint.\n",
    "    :param paramsWithoutToken: Dictionary containing the request parameters without the token. Defaults to paramsDefault.\n",
    "    :param token: String containing the authentication token. Defaults to a fixed token.\n",
    "    \n",
    "    :return: A pandas DataFrame containing the data fetched from the API.\n",
    "    \n",
    "    :raises: May propagate exceptions related to the request if they occur.\n",
    "    \"\"\"\n",
    "        \n",
    "    url = f'http://montreal.icea.decea.mil.br:5002/api/v1/{route}'\n",
    "    params = paramsWithoutToken\n",
    "    params['token'] = token\n",
    "    data = None\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = req.get(url, params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            break\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def dropOutlier(X, name_cols, lower_percentile=0.05, upper_percentile=0.95):\n",
    "    X['route'] = X['origem'] + '_' + X['destino']  \n",
    "    keep_rows = pd.Series([True] * X.shape[0], index=X.index)\n",
    "    \n",
    "    for route in X['route'].unique():\n",
    "    \n",
    "        for col in name_cols:\n",
    "            lower_bound = X[X['route'] == route][col].quantile(lower_percentile)\n",
    "            upper_bound = X[X['route'] == route][col].quantile(upper_percentile)\n",
    "            keep_rows &= (X['route'] != route) | ((X[col] >= lower_bound) & (X[col] <= upper_bound))\n",
    "        \n",
    "    X.drop('route', axis = 1, inplace = True)\n",
    "\n",
    "    return X[keep_rows]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create estimate record based on route and reference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EstimatedDuration(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Estimator that calculates the estimated duration based on route and time of day.\n",
    "    \n",
    "    Attributes:\n",
    "        _dammy_estimator (DataFrame): Table with average durations by route and time of day.\n",
    "        _selected_cols (list): Selected columns from the DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, _selected_cols, _dammy_estimator = None):\n",
    "        \"\"\"\n",
    "        Initializes the estimator with selected columns and optionally a dummy estimator.\n",
    "        \n",
    "        :param _selected_cols: List of selected columns.\n",
    "        :param _dammy_estimator: Optional dummy estimator. Default is None.\n",
    "        \"\"\"\n",
    "        self._dammy_estimator = _dammy_estimator\n",
    "        self._selected_cols = _selected_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the estimator to the data set.\n",
    "        \n",
    "        :param X: Input DataFrame.\n",
    "        :param y: Series with durations. Default is None.\n",
    "        :return: Returns the fitted object.\n",
    "        \"\"\"\n",
    "        \n",
    "        if y is not None:\n",
    "            X = pd.DataFrame(X, columns = self._selected_cols)\n",
    "            X['duration'] = y.values\n",
    "            X['route'] = X['origem'] + '_' + X['destino']            \n",
    "            X['only_hour'] = X.apply(lambda row: self._gethourFromDatetime(row), axis = 1)\n",
    "\n",
    "            avarageRouteHour = X.groupby(['route', 'only_hour']).agg({'duration': ['mean']})['duration'].reset_index()\n",
    "            avarageRoute = X.groupby(['route']).agg({'duration': ['mean']})['duration'].reset_index()           \n",
    "            avarageRouteHour.columns = ['route', 'only_hour', 'mean_hour']\n",
    "            avarageRoute.columns = ['route', 'mean_absolute']\n",
    "            self._dammy_estimator = avarageRouteHour.merge(avarageRoute, on=['route'], how='left')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transforms the DataFrame X based on the estimated duration.\n",
    "        \n",
    "        :param X: Input DataFrame.\n",
    "        :param y: Ignored.\n",
    "        :return: Transformed DataFrame with estimated duration.\n",
    "        \"\"\"\n",
    "        \n",
    "        X = pd.DataFrame(X, columns = self._selected_cols)\n",
    "        X['route'] = X['origem'] + '_' + X['destino']\n",
    "        X['only_hour'] = X.apply(lambda row: self._gethourFromDatetime(row), axis = 1)\n",
    "        X['estimated_duration'] = X.apply(lambda row: self._dammy_predict(row['route'], row['only_hour']), axis = 1)\n",
    "        X.drop('route', axis = 1, inplace = True)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _dammy_predict(self, route, hour):\n",
    "        \"\"\"\n",
    "        Predicts the duration based on route and time of day.\n",
    "        \n",
    "        :param route: Desired route.\n",
    "        :param hour: Hour of the day.\n",
    "        :return: Estimated duration.\n",
    "        \"\"\"\n",
    "            \n",
    "        try:\n",
    "            return self._dammy_estimator[\n",
    "                (self._dammy_estimator['route'] == route) & (self._dammy_estimator['only_hour'] == hour)\n",
    "            ].iloc[0, 2]\n",
    "        \n",
    "        except:\n",
    "            try:\n",
    "                return self._dammy_estimator[self._dammy_estimator['route'] == route].iloc[0, 3]\n",
    "            except:\n",
    "                return 0\n",
    "    \n",
    "    def _gethourFromDatetime(self, row):\n",
    "        \"\"\"\n",
    "        Extracts the hour from a datetime column.\n",
    "        \n",
    "        :param row: Row from the DataFrame.\n",
    "        :return: Extracted hour.\n",
    "        \"\"\"\n",
    "            \n",
    "        try:\n",
    "            return datetime.strptime(row['hora_ref'], \"%Y-%m-%d %H:%M:%S\").hour\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                return datetime.strptime(row['hora_ref'], \"%Y-%m-%d %H:%M:%S.%f\").hour\n",
    "            \n",
    "            except:\n",
    "                return datetime.strptime(row['hora_ref'], \"%H:%M:%S\").hour\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"\n",
    "        Returns the estimator's parameters.\n",
    "        \n",
    "        :param deep: Ignored. Retained for compatibility with sklearn's interface.\n",
    "        :return: Dictionary with the parameters.\n",
    "        \"\"\"\n",
    "            \n",
    "        return {\n",
    "            \"_dammy_estimator\": self._dammy_estimator,\n",
    "            \"_selected_cols\": self._selected_cols\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"\n",
    "        Sets the estimator's parameters.\n",
    "        \n",
    "        :param params: Dictionary with the new parameters.\n",
    "        :return: Returns the object with updated parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        valid_params = self.get_params(deep=True)\n",
    "        \n",
    "        for key, value in params.items():\n",
    "            if key not in valid_params:\n",
    "                raise ValueError(f\"Invalid parameter {key} for estimator {self.__class__.__name__}. Check the list of available parameters with `estimator.get_params().keys()`.\")\n",
    "            \n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtains geometric information (linearity) of the path taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnapshotRadarHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Handler for Radar Snapshots (SnapshotRadar).\n",
    "\n",
    "    This class is responsible for processing radar trajectory data and \n",
    "    deriving features such as route linearity. It inherits functionalities \n",
    "    from the BaseEstimator and TransformerMixin classes of Scikit-Learn.\n",
    "\n",
    "    Methods:\n",
    "    - fit: Sets the internal state of the transformer based on the input data.\n",
    "    - transform: Applies transformations on the input data, producing new features.\n",
    "    - _haversine_distance: Calculates the straight-line distance between two geographical points.\n",
    "\n",
    "    Note: Some functionalities and calculations, such as direct distance and \n",
    "    travelled distance, are commented out in the code. They can be uncommented \n",
    "    and used as needed.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "  \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transforms the input data by adding the route linearity metric.\n",
    "\n",
    "        Parameters:\n",
    "        - X : DataFrame\n",
    "            Input data containing the 'snapshot_radar' column with coordinates.\n",
    "        - y : array-like, default=None\n",
    "            Target labels. Not used, but retained for compatibility.\n",
    "\n",
    "        Returns:\n",
    "        - X : DataFrame\n",
    "            Transformed input data with an additional 'route_linearity' column.\n",
    "        \"\"\"\n",
    "        route_linearity_aux = []\n",
    "\n",
    "        for _, row in X.iterrows():\n",
    "            multipoint_str = row['snapshot_radar']\n",
    "\n",
    "            coords = re.findall(r'(-?\\d+\\.\\d+) (-?\\d+\\.\\d+)', multipoint_str)\n",
    "            points_rad = np.array(coords, dtype=float)\n",
    "            points = np.degrees(points_rad)\n",
    "\n",
    "            if len(points) < 2:\n",
    "\n",
    "                direct_distance = 1200\n",
    "                travelled_distance = 1200\n",
    "                route_linearity = 1.0\n",
    "\n",
    "            else:\n",
    "                lon = points[:, 0]\n",
    "                lat = points[:, 1]\n",
    "\n",
    "                # Split points into X and y\n",
    "                _X = lat.reshape(-1, 1)\n",
    "                _y = lon\n",
    "\n",
    "                # Create, fit the model and return the R^2 value        \n",
    "                score = LinearRegression().fit(_X, _y).score(_X, _y)\n",
    "                route_linearity = score\n",
    "\n",
    "                plot = False \n",
    "\n",
    "                if plot:\n",
    "                    # Criando figura e eixos\n",
    "                    fig, ax = plt.subplots(figsize=(7, 7), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "                    ax.set_extent([-55, -33, -32, 0])\n",
    "                    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "                    ax.add_feature(cfeature.COASTLINE)\n",
    "                    ax.add_feature(cfeature.LAND, edgecolor='black')                    \n",
    "                    ax.set_title(\n",
    "                        f\"Coefficient of Determination (R²): {score:.3f}\"\n",
    "                    )\n",
    "                    \n",
    "                    for n in range(len(lat)):\n",
    "                        ax.plot(lon[n], lat[n], 'ro', markersize=1)    \n",
    "\n",
    "                    plt.show()\n",
    "                    plt.close()\n",
    "\n",
    "            route_linearity_aux.append(route_linearity)\n",
    "\n",
    "        X['route_linearity'] = route_linearity_aux\n",
    "\n",
    "        X.drop('snapshot_radar', axis = 1, inplace = True)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features based on METAR translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetarHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer that translates METAR information into numerical features.\n",
    "    \n",
    "    This transformer extracts specific METAR information and adds corresponding \n",
    "    numerical features to the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the transformer to the dataset. In this case, the method is passive and doesn't perform operations.\n",
    "        \n",
    "        :param X: Input DataFrame.\n",
    "        :param y: Ignored.\n",
    "        :return: Returns the fitted object.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transforms the DataFrame X by extracting METAR information and converting it into numerical features.\n",
    "        \n",
    "        :param X: Input DataFrame containing a 'metar' column with METAR information.\n",
    "        :param y: Ignored.\n",
    "        :return: Transformed DataFrame with numerical features derived from METAR information.\n",
    "        \"\"\"\n",
    "        \n",
    "        X['wind_variation'] = 0.0\n",
    "        X['qnh'] = 0.0\n",
    "        X[\"cavok\"] = 0.0\n",
    "        X[\"gust\"] = 0.0\n",
    "        X[\"FEW_presenceCB\"] = 0.0\n",
    "\n",
    "        for index, row in X.iterrows():\n",
    "            translationMetar = Metar(row['destino'], row['metar'])\n",
    "            X.loc[index, 'cavok'] = float(bool(re.search('CAVOK', row['metar'])))\n",
    "\n",
    "            if(translationMetar.wind):\n",
    "                X.loc[index, 'wind_variation'] = float(translationMetar.wind['variation'] != None)\n",
    "                X.loc[index, 'gust'] = float(translationMetar.wind['gust'] != None)\n",
    "            else:\n",
    "                # Se não tem informação de vento significa METAR no formato ex.: VRB07G17KT (vento variavel com rajada)\n",
    "                X.loc[index, 'gust'] = 1\n",
    "\n",
    "            if(translationMetar.qnh):\n",
    "                X.loc[index, 'qnh'] = float(translationMetar.qnh)\n",
    "\n",
    "            if(translationMetar.cloud):\n",
    "                for formation in translationMetar.cloud:\n",
    "                    if(formation['code'] == 'FEW'):\n",
    "                        X.loc[index, \"FEW_presenceCB\"] = float(formation['presenceCB']) \n",
    "                    \n",
    "        X.drop('metar', axis = 1, inplace = True)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtains meteorology image of the flight path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteImageHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Handler for satellite images for feature extraction and transformation.\n",
    "    \n",
    "    This transformer processes satellite images, extracts specific information, \n",
    "    and converts that information into numerical features in a DataFrame.\n",
    "    \n",
    "    Attributes:\n",
    "        _restore (str): Path for route image data.\n",
    "        _width (int): Width of the image.\n",
    "        _outputWidth (int): Output image width.\n",
    "        _outputHeight (int): Output image height.\n",
    "        _printRoutes (bool): Indicates if the routes should be printed.\n",
    "        _printImage (bool): Indicates if the image should be printed.\n",
    "        _imagesRouteData (DataFrame): Route image data.\n",
    "        _itineraries (list): List of itineraries.\n",
    "        _date (dict): Date range.\n",
    "        _location (dict): Airport locations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, _restore=None, _width=50, _outputWidth=128,\n",
    "                 _outputHeight=32, _printRoutes=False, _printImage=False):\n",
    "        \"\"\"\n",
    "        Initializes the image handler with the specified parameters.\n",
    "        \n",
    "        :param _restore: Path to restore the route image data.\n",
    "        :param _width: Width of the image.\n",
    "        :param _outputWidth: Output image width.\n",
    "        :param _outputHeight: Output image height.\n",
    "        :param _printRoutes: Whether the routes should be printed.\n",
    "        :param _printImage: Whether the image should be printed.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._restore = _restore\n",
    "        self._width = _width\n",
    "        self._outputWidth = _outputWidth\n",
    "        self._outputHeight = _outputHeight\n",
    "        self._printRoutes = _printRoutes\n",
    "        self._printImage = _printImage\n",
    "        self._imagesRouteData = None\n",
    "        self._itineraries = []\n",
    "        self._date = paramsDefault = {\n",
    "            'idate': '2022-06-02', \n",
    "            'fdate': '2023-05-11'\n",
    "        }          \n",
    "        \n",
    "        self._location = {\n",
    "            'SBGR': [1665, 1459],\n",
    "            'SBCF': [1722, 1376],\n",
    "            'SBRJ': [1739, 1447],\n",
    "            'SBPA': [1557, 1614],\n",
    "            'SBSV': [1853, 1217],\n",
    "            'SBFL': [1618, 1562],\n",
    "            'SBRF': [1932, 1106],\n",
    "            'SBBR': [1632, 1286],\n",
    "            'SBCT': [1599, 1509],\n",
    "            'SBSP': [1662, 1469],\n",
    "            'SBKP': [1648, 1453],\n",
    "            'SBGL': [1740, 1446]\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for aero_from in self._location.keys():\n",
    "            for aero_to in self._location.keys():\n",
    "                if aero_from == aero_to:\n",
    "                    continue\n",
    "                    \n",
    "                self._itineraries.append(f\"{aero_from}_{aero_to}\")\n",
    "                                                    \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the transformer to the dataset. The method loads the route image data if specified.\n",
    "        \n",
    "        :param X: Input DataFrame.\n",
    "        :param y: Ignored.\n",
    "        :return: Returns the fitted object.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self._restore:\n",
    "            self._imagesRouteData = pd.read_csv(self._restore)\n",
    "            return self\n",
    "        \n",
    "        route = []\n",
    "        hora_ref = []\n",
    "        imageSatelite_red = []\n",
    "        imageSatelite_yellow = []\n",
    "        imageSatelite_green = []\n",
    "        imageSatelite_blue = []\n",
    "\n",
    "        df_sat_met = getData(route = 'satelite', paramsWithoutToken = self._date)\n",
    "\n",
    "        firstCicle = True\n",
    "        \n",
    "        for _, row in df_sat_met.iterrows():\n",
    "                       \n",
    "            print(f\"date: {row['data']}\")\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    response = req.get(row['path']).content\n",
    "                    break\n",
    "                except:\n",
    "                    print(f\"requisição falhou em {row['data']}. Aguardar 5 segs e retomar deste ponto\")\n",
    "                    sleep(5)\n",
    "\n",
    "            arrayImage = np.asarray(bytearray(response), dtype=np.uint8)\n",
    "            cv2ImageBGR = cv2.imdecode(arrayImage, cv2.IMREAD_COLOR)\n",
    "\n",
    "            # Converta a imagem para escala de cinza\n",
    "            cv2ImageRGB = cv2.cvtColor(cv2ImageBGR, cv2.COLOR_BGR2RGB)\n",
    "          \n",
    "            if firstCicle:\n",
    "                cv2ImageRGBCopy = cv2ImageRGB.copy()\n",
    "\n",
    "            for itinerary in self._itineraries:        \n",
    "                [origin, destiny] = itinerary.split('_')\n",
    "                p_from = np.array(self._location[origin])\n",
    "                p_to = np.array(self._location[destiny])\n",
    "\n",
    "                # Vetor direcional\n",
    "                v = p_to - p_from\n",
    "                v = v / np.linalg.norm(v)  # Normaliza o vetor\n",
    "\n",
    "                # Vetor perpendicular\n",
    "                v_perp = np.array([v[1], -v[0]])\n",
    "\n",
    "                # Pontos do quadrilátero usando o padding para determinar as extremidades\n",
    "                padding = self._width/2\n",
    "                p2 = p_to + v * padding + v_perp * padding\n",
    "                p3 = p_to + v * padding - v_perp * padding\n",
    "                p1 = p_from - v * padding + v_perp * padding\n",
    "                p4 = p_from - v * padding - v_perp * padding\n",
    "\n",
    "                if firstCicle:\n",
    "                    pts = np.array([p1, p2, p3, p4], np.int32)\n",
    "                    pts = pts.reshape((-1, 1, 2))\n",
    "\n",
    "                    # Desenhe o paralelogramo vermelho\n",
    "                    color = (255, 0, 0)  # Vermelho em RGB\n",
    "                    thickness = 4  # Define a espessura da linha do paralelogramo\n",
    "                    cv2.polylines(cv2ImageRGBCopy, [pts], isClosed=True, color=color, thickness=thickness)\n",
    "\n",
    "\n",
    "\n",
    "                satelitePoints = np.array([p1, p2, p3, p4], dtype=np.float32)\n",
    "                outputPoints = np.array([[0, 0], [self._outputWidth, 0],\n",
    "                                         [self._outputWidth, self._outputHeight],\n",
    "                                         [0, self._outputHeight]], dtype=np.float32)\n",
    "\n",
    "                # Calcula a matriz de transformação de perspectiva\n",
    "                matriz = cv2.getPerspectiveTransform(satelitePoints, outputPoints)\n",
    "\n",
    "                # Realiza a transformação de perspectiva\n",
    "                outputImage = cv2.warpPerspective(cv2ImageRGB, matriz, (self._outputWidth, self._outputHeight))\n",
    "\n",
    "                arrayImageOutput = np.array(outputImage)\n",
    "\n",
    "                hsv = cv2.cvtColor(arrayImageOutput, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "                red_lower_limit1 = np.array([0, 50, 50])\n",
    "                red_upper_limit1 = np.array([12, 255, 255])\n",
    "                red_lower_limit2 = np.array([150, 50, 50])\n",
    "                red_upper_limit2 = np.array([180, 255, 255])\n",
    "                mask_red1 = cv2.inRange(hsv, red_lower_limit1, red_upper_limit1)\n",
    "                mask_red2 = cv2.inRange(hsv, red_lower_limit2, red_upper_limit2)\n",
    "                mask_red = cv2.bitwise_or(mask_red1, mask_red2)\n",
    "                outputImage_red = cv2.bitwise_and(arrayImageOutput, arrayImageOutput, mask=mask_red)\n",
    "                mean_r = outputImage_red.mean()\n",
    "\n",
    "                yellow_lower_limit1 = np.array([22, 50, 50])\n",
    "                yellow_upper_limit1 = np.array([30, 255, 255])\n",
    "                yellow_lower_limit2 = np.array([31, 50, 50])\n",
    "                yellow_upper_limit2 = np.array([38, 255, 255])\n",
    "                mask_yellow1 = cv2.inRange(hsv, yellow_lower_limit1, yellow_upper_limit1)\n",
    "                mask_yellow2 = cv2.inRange(hsv, yellow_lower_limit2, yellow_upper_limit2)\n",
    "                mask_yellow = cv2.bitwise_or(mask_yellow1, mask_yellow2)\n",
    "                outputImage_yellow = cv2.bitwise_and(arrayImageOutput, arrayImageOutput, mask=mask_yellow)\n",
    "                mean_y = outputImage_yellow.mean()\n",
    "\n",
    "                green_lower_limit1 = np.array([40, 50, 50])\n",
    "                green_upper_limit1 = np.array([70, 255, 255])\n",
    "                green_lower_limit2 = np.array([71, 50, 50])\n",
    "                green_upper_limit2 = np.array([80, 255, 255])\n",
    "                mask_green1 = cv2.inRange(hsv, green_lower_limit1, green_upper_limit1)\n",
    "                mask_green2 = cv2.inRange(hsv, green_lower_limit2, green_upper_limit2)\n",
    "                mask_green = cv2.bitwise_or(mask_green1, mask_green2)\n",
    "                outputImage_green = cv2.bitwise_and(arrayImageOutput, arrayImageOutput, mask=mask_green)\n",
    "                mean_g = outputImage_green.mean()\n",
    "\n",
    "                blue_lower_limit1 = np.array([100, 50, 50])\n",
    "                blue_upper_limit1 = np.array([120, 255, 255])\n",
    "                blue_lower_limit2 = np.array([121, 50, 50])\n",
    "                blue_upper_limit2 = np.array([140, 255, 255])\n",
    "                mask_blue1 = cv2.inRange(hsv, blue_lower_limit1, blue_upper_limit1)\n",
    "                mask_blue2 = cv2.inRange(hsv, blue_lower_limit2, blue_upper_limit2)\n",
    "                mask_blue = cv2.bitwise_or(mask_blue1, mask_blue2)\n",
    "                outputImage_blue = cv2.bitwise_and(arrayImageOutput, arrayImageOutput, mask=mask_blue)\n",
    "                mean_b = outputImage_blue.mean()\n",
    "\n",
    "                route.append(itinerary)\n",
    "                hora_ref.append(row['data'])\n",
    "                imageSatelite_red.append(mean_r)\n",
    "                imageSatelite_yellow.append(mean_y)\n",
    "                imageSatelite_green.append(mean_g)\n",
    "                imageSatelite_blue.append(mean_b)\n",
    "\n",
    "                if self._printImage:\n",
    "                    gap_size = 1\n",
    "                    arrayImageOutput_with_gap = self._add_gap(arrayImageOutput, gap_size)\n",
    "                    outputImage_red_with_gap = self._add_gap(outputImage_red, gap_size)\n",
    "                    outputImage_yellow_with_gap = self._add_gap(outputImage_yellow, gap_size)\n",
    "                    outputImage_green_with_gap = self._add_gap(outputImage_green, gap_size)\n",
    "\n",
    "                    # Concatenar as imagens verticalmente com espaços\n",
    "                    concatenated_image = np.vstack((arrayImageOutput_with_gap, outputImage_red_with_gap, outputImage_yellow_with_gap, outputImage_green_with_gap, outputImage_blue))\n",
    "\n",
    "                    # Exibir a imagem resultante\n",
    "                    plt.figure(figsize=(10,50)) # 5, 25\n",
    "                    plt.imshow(concatenated_image)\n",
    "                    plt.title(f\"{itinerary} - {row['data']} r: {mean_r: .1f} y: {mean_y: .1f} g: {mean_g: .1f} b: {mean_b: .1f}\", fontsize=6)\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "            if firstCicle and self._printRoutes:\n",
    "                plt.figure(figsize=(16, 16)) # 8,8\n",
    "                plt.imshow(cv2ImageRGBCopy)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "                firstCicle = False\n",
    "\n",
    "        self._imagesRouteData = pd.DataFrame({\n",
    "            'route': route,\n",
    "            'hora_ref': hora_ref, \n",
    "            'imageSatelite_red': imageSatelite_red,\n",
    "            'imageSatelite_yellow': imageSatelite_yellow,\n",
    "            'imageSatelite_green': imageSatelite_green,\n",
    "            'imageSatelite_blue': imageSatelite_blue\n",
    "        })\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X['route'] = X['origem'] + '_' + X['destino']\n",
    "        X = X.merge(self._imagesRouteData, on=['hora_ref', 'route'], how='left')\n",
    "        X.fillna(0, inplace=True)\n",
    "        X.drop(['route', 'path', 'hora_ref'], axis = 1, inplace = True)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _add_gap(self, image, gap_size, gap_color=[255, 255, 255]):\n",
    "        return np.vstack((image, np.full((gap_size, image.shape[1], 3), gap_color, dtype=np.uint8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MXGB Remove Constant Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveConstantAttribute(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):               \n",
    "        X.drop(['origem', 'destino'], axis = 1, inplace = True)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Boxtplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duration_boxplot(data, x_col, y_col, figsize=(8, 50)):\n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    sns.boxplot(   \n",
    "        data=data, x=x_col, y=y_col,   \n",
    "        showcaps=True,\n",
    "        flierprops={\"marker\": \"x\"},\n",
    "        boxprops={\"facecolor\": (.3, .5, .7, .5)},\n",
    "        medianprops={\"color\": \"r\", \"linewidth\": 1},\n",
    "        whis=1.5\n",
    "    )\n",
    "\n",
    "    ax.xaxis.grid(True)\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    ax.set(ylabel=\"\")\n",
    "    sns.despine(trim=True, left=True)\n",
    "    plt.legend([], [], frameon=False)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def adjust_timestamp(ts):\n",
    "    timestamp_s = ts / 1000\n",
    "    dt = datetime.utcfromtimestamp(timestamp_s)\n",
    "    return dt.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "def timestamp_to_string(timestamp):\n",
    "    # Convert the timestamp to a datetime object\n",
    "    dt_object = datetime.fromtimestamp(timestamp)\n",
    "    \n",
    "    # Format the datetime object as a string\n",
    "    formatted_string = dt_object.strftime('%Y-%m-%d %H:%M:%S') + '.' + '{:03d}'.format(int(dt_object.microsecond / 1000))\n",
    "    \n",
    "    return formatted_string\n",
    "\n",
    "def string_to_timestamp(date_str):\n",
    "    # Convert the string to a datetime object\n",
    "    dt_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Convert the datetime object to a timestamp\n",
    "    timestamp = dt_object.timestamp()\n",
    "    \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            'imputer_cat',\n",
    "            SimpleImputer(strategy='most_frequent'),\n",
    "            ['origem', 'destino', 'metar', 'hora_ref', 'path', 'snapshot_radar']\n",
    "        ),\n",
    "        (\n",
    "            'imputer_num',\n",
    "            SimpleImputer(strategy='constant', fill_value=0),\n",
    "            ['troca', 'esperas']\n",
    "        )\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineer = Pipeline(steps=[\n",
    "    ('feat_eng_est_duration', EstimatedDuration(selected_cols)),\n",
    "    ('feat_eng_snapshot_radar', SnapshotRadarHandler()),\n",
    "    ('feat_eng_metar', MetarHandler()),\n",
    "    ('feat_eng_satelliteImage', SatelliteImageHandler(_restore='../data/imagesRouteData.csv'))    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_category_var = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            'coding_cat',\n",
    "            OneHotEncoder(handle_unknown='ignore'),\n",
    "            ['origem', 'destino']\n",
    "        ),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Pipeline(steps=[\n",
    "    ('trans_stand', StandardScaler(with_mean=True)),\n",
    "    # ('trans_norm', Normalizer()),\n",
    "    # ('trans_pca', PCA(n_components=pca_n_components))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(steps=[\n",
    "    ('imputer', imputer),\n",
    "    ('feature_engineer', feature_engineer),\n",
    "    ('coding_category_var', coding_category_var),\n",
    "    ('transformer', transformer)\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_origin = pd.read_csv(\"../data/idsc_train.csv\", index_col='flightid')\n",
    "\n",
    "data_train_origin.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_without_outlier = dropOutlier(\n",
    "    data_train_origin,\n",
    "    name_cols = ['duration'],\n",
    "    lower_percentile=0.1,\n",
    "    upper_percentile=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, _ = train_test_split(\n",
    "    data_train_without_outlier,\n",
    "    train_size=data_train_without_outlier.shape[0] * training_sample_size -1,\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from predictors\n",
    "y = data_train.duration\n",
    "X = data_train.drop(['duration'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "# Keep selected columns only\n",
    "X_train = X_train_full[selected_cols].copy()\n",
    "X_valid = X_valid_full[selected_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For clarity, the pipeline will be applied to the training data in intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_origin = data_train_origin.copy()\n",
    "aux_origin['route'] = aux_origin['origem'] + '_' + aux_origin['destino']\n",
    "\n",
    "plot_duration_boxplot(aux_origin, \"duration\", \"route\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "step_a = preprocessor.named_steps['imputer'].fit_transform(X_train, y_train)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Transforming Time: {(end-start)/60:.2f} minutes\")\n",
    "\n",
    "pd.DataFrame(step_a, columns = X_train.columns).sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(step_a, columns = X_train.columns).isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an estimate duration feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "step_b = preprocessor.named_steps['feature_engineer'].named_steps['feat_eng_est_duration'].fit_transform(step_a, y_train)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Transforming Time: {(end-start)/60:.2f} minutes\")\n",
    "\n",
    "step_b.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the \"route_linearity\" attribute (CAT-62 Feature Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "step_c = preprocessor.named_steps['feature_engineer'].named_steps['feat_eng_snapshot_radar'].fit_transform(step_b, y_train)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Transforming Time: {(end-start)/60:.2f} minutes\")\n",
    "\n",
    "step_c.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the \"wind_variation\", \"qnh\", \"cavok\", \"gust\" and \"FEW_presenceCB\" attribute (METAR Feature Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "step_d = preprocessor.named_steps['feature_engineer'].named_steps['feat_eng_metar'].fit_transform(step_c, y_train)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Transforming Time: {(end-start)/60:.2f} minutes\")\n",
    "\n",
    "step_d.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the \"imageSatelite_red\", \"imageSatelite_yellow\", \"imageSatelite_green\" and \"imageSatelite_blue\" attribute (meteorological satellite Feature Engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "step_e = preprocessor.named_steps['feature_engineer'].named_steps['feat_eng_satelliteImage'].fit_transform(step_d, y_train)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Transforming Time: {(end-start)/60:.2f} minutes\")\n",
    "\n",
    "step_e.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coding of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "step_f = preprocessor.named_steps['coding_category_var'].fit_transform(step_e, y_train)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Transforming Time: {(end-start)/60:.2f} minutes\")\n",
    "\n",
    "pd.DataFrame(step_f).sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardization of variables to the same scale (unit standard deviation and zero mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "_X_train = preprocessor.named_steps['transformer'].fit_transform(step_f, y_train)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Transforming Time: {(end-start)/60:.2f} minutes\")\n",
    "\n",
    "pd.DataFrame(_X_train).sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "_X_valid = preprocessor.transform(X_valid)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Transforming Time: {(end-start)/60:.2f} minutes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_test = '../data/idsc_dataset.csv'\n",
    "\n",
    "data_test = pd.read_csv(path_data_test, index_col = 'flightid', delimiter=';')\n",
    "\n",
    "X_test = data_test[selected_cols].copy()\n",
    "\n",
    "start = time()\n",
    "\n",
    "_X_test = preprocessor.transform(X_test)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Transforming Time: {(end-start)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummyModel = preprocessor.named_steps['feature_engineer'].named_steps['feat_eng_est_duration']._dammy_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gethourFromDatetime = preprocessor.named_steps['feature_engineer'].named_steps['feat_eng_est_duration']._gethourFromDatetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = X_test.copy().reset_index()\n",
    "\n",
    "aux['route'] = aux['origem'] + '_' + aux['destino']\n",
    "aux['estimated'] = None\n",
    "aux['only_hour'] = aux.apply(lambda row: gethourFromDatetime(row), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in aux.iterrows():\n",
    "    try:\n",
    "        mean_hour = dummyModel[(dummyModel['route'] == row['route']) & (dummyModel['only_hour'] == row['only_hour'])]['mean_hour']\n",
    "        mean_absolute = dummyModel[(dummyModel['route'] == row['route'])].iloc[0, 3]\n",
    "    except:\n",
    "        mean_absolute = 0\n",
    "    \n",
    "    if(np.isnan(mean_hour).any()):\n",
    "        aux.loc[index, 'estimated'] = mean_hour\n",
    "    else:\n",
    "        aux.loc[index, 'estimated'] = mean_absolute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dummy = aux[['flightid', 'estimated']]\n",
    "\n",
    "submission_dummy.columns = ['ID', 'solution']\n",
    "submission_dummy.to_csv(f\"../data/submission/idsc_submission_dummy{datetime.now().strftime('%d-%B-%Ih%Mmin')}.csv\", index=False)\n",
    "\n",
    "submission_dummy.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape = (_X_train.shape[1],),\n",
    "                 neuron_number = 128,\n",
    "                 optimizer = 'adam',\n",
    "                 checkpoint = None,\n",
    "                 activation ='relu',\n",
    "                 layers=1,\n",
    "                 dropout=0\n",
    "                ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Constructs a neural network model with specified parameters.\n",
    "    \n",
    "    :param input_shape: tuple, shape of the input data. Default is (37,).\n",
    "    :param neuron_number: int, number of neurons in the dense layer(s). Default is 128.\n",
    "    :param optimizer: str, optimizer used in model compilation. Default is 'adam'.\n",
    "    :param checkpoint: str, path to the model checkpoint to load weights from. If the path exists, the model will load the weights. Default is None.\n",
    "    :param activation: str, activation function used in the dense layers. Default is 'relu'.\n",
    "    :param layers: int, number of dense layers to be added before the output layer. Default is 1.\n",
    "    :param dropout: float, dropout rate to be applied after the dense layers. Value should be between 0 (no dropout) and 1 (full dropout). Default is 0.\n",
    "    \n",
    "    :return: Sequential, a Keras Sequential model constructed based on the provided parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "                    \n",
    "    #################################################################\n",
    "                    \n",
    "    for _ in range(layers):\n",
    "        model.add(Dense(neuron_number, activation=activation ))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "    #################################################################\n",
    "                        \n",
    "    model.add(Dense(neuron_number//2, activation=activation ))\n",
    "    model.add(Dense(1, activation='linear' ))\n",
    "\n",
    "    if(checkpoint != None) and (os.path.exists(checkpoint)):\n",
    "        model.load_weights(checkpoint)\n",
    "        \n",
    "    model.compile(loss= MeanSquaredError(), optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir=f\"../logs/{datetime.now().strftime('%d-%B-%Ih%Mmin')}\",\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir=f\"../logs/{datetime.now().strftime('%d-%B-%Ih%Mmin')}\",\n",
    "    histogram_freq=0,\n",
    "    write_graph=False,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq=\"epoch\",\n",
    "    profile_batch=0,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '../model/checkpoints',\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=verbose,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"min\",\n",
    "    save_freq=\"epoch\"\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=20,\n",
    "    verbose=verbose,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduceLr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    mode=\"min\",\n",
    "    verbose=verbose,\n",
    "    min_delta=0.0001,\n",
    "    min_lr=0\n",
    ")\n",
    "\n",
    "BackAndRes = BackupAndRestore(\n",
    "    backup_dir = '../tmp',\n",
    "    save_freq=\"epoch\",\n",
    "    delete_checkpoint=True,\n",
    "    save_before_preemption=False\n",
    ")\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint,\n",
    "    earlystop,\n",
    "    tensorboard,\n",
    "    reduceLr,\n",
    "    BackAndRes,\n",
    "    TerminateOnNaN()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_params = {\n",
    "    'checkpoint': None,\n",
    "    'neuron_number': 128,\n",
    "    'activation': 'relu',\n",
    "    'model__layers': 3,\n",
    "    'model__dropout': 0.05\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model with KerasClassifier\n",
    "ann_model = KerasRegressor(model=create_model, callbacks=callbacks, **create_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_regressor = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', ann_model)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_param_grid = {\n",
    "    'model__batch_size': [32],\n",
    "    'model__optimizer': [\n",
    "        Adam(learning_rate=0.01),\n",
    "        # RMSprop(learning_rate=0.01),\n",
    "        # SGD(learning_rate=0.01),\n",
    "        # Adamax(learning_rate=0.01),\n",
    "        # Adadelta(learning_rate=0.01),\n",
    "        # Adagrad(learning_rate=0.01)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_fit_params = {\n",
    "    'model__epochs': ann_epochs,\n",
    "    'model__verbose': verbose,\n",
    "    'model__shuffle': True,\n",
    "    'model__steps_per_epoch': None,\n",
    "    'model__validation_data': (_X_valid, y_valid),\n",
    "    'model__validation_steps': None,\n",
    "    'model__validation_batch_size': None,\n",
    "    'model__validation_freq': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_gridSearch = GridSearchCV(\n",
    "    estimator = ann_regressor,\n",
    "    param_grid = ann_param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    refit=True,\n",
    "    cv= fold_cross_validation,\n",
    "    verbose=2#verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "ann_gridSearch.fit(X=X_train, y=y_train, **ann_fit_params)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Training Time: {(end-start)/60:.2f} minutes\")\n",
    "\n",
    "pd.DataFrame(ann_gridSearch.cv_results_)[[\n",
    "    'rank_test_score', 'param_model__batch_size', 'param_model__optimizer',\n",
    "    'mean_test_score', 'std_test_score', 'mean_fit_time'\n",
    "]].sort_values(by='rank_test_score').set_index('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_best_model = ann_gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ann_best_model, '../model/ann_best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {\n",
    "    'xgb__booster': ['gbtree'], # 'gbtree', 'dart', 'gblinear'\n",
    "    'xgb__n_estimators': [xgb_estimators], # normalmente entre 50 e 1000\n",
    "    \n",
    "    ##### Parâmetros para Tree Booster (booster = gbtree) ##########\n",
    "       \n",
    "    'xgb__learning_rate': [0.1], # de 0 a 1\n",
    "    'xgb__gamma': [0],\n",
    "    'xgb__max_depth': [5],\n",
    "    'xgb__min_child_weight': [5], # 1\n",
    "    'xgb__max_delta_step': [0],\n",
    "    'xgb__subsample': [1],\n",
    "    'xgb__sampling_method': ['uniform'], # 'uniform', 'gradient_based', \n",
    "    'xgb__colsample_bytree': [1], # de 0 a 1\n",
    "    'xgb__colsample_bylevel': [1], # de 0 a 1\n",
    "    'xgb__colsample_bynode': [1], # de 0 a 1\n",
    "    'xgb__scale_pos_weight': [1],\n",
    "    'xgb__grow_policy': ['depthwise'], # 'depthwise', 'lossguide'\n",
    "    'xgb__max_leaves': [0],\n",
    "    'xgb__max_bin': [256],\n",
    "    'xgb__num_parallel_tree': [1],\n",
    "    'xgb__refresh_leaf': [1],\n",
    "    \n",
    "    #################################################################\n",
    "    \n",
    "    # Parâmetros para Dart Booster (booster = dart) #################\n",
    "    \n",
    "    'xgb__sample_type': ['uniform'], # 'uniform', 'weighted'\n",
    "    'xgb__normalize_type': ['tree'], # 'tree', 'forest'\n",
    "    'xgb__rate_drop': [0], # de 0 a 1\n",
    "    'xgb__one_drop': [0], \n",
    "    'xgb__skip_drop': [0],\n",
    "    \n",
    "    #################################################################\n",
    "    \n",
    "    # Parâmetros para Linear Booster (booster = gblinear) ###########\n",
    "\n",
    "    'xgb__feature_selector': ['cyclic'], # 'cyclic', 'shuffle', 'random', 'greedy', 'thrifty',  \n",
    "    'xgb__top_k': [0]\n",
    "    \n",
    "    #################################################################   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_fit_params = {\n",
    "    'xgb__eval_set': [(_X_valid, y_valid)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridseach and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gridSearch = GridSearchCV(\n",
    "    estimator = xgb_regressor,\n",
    "    param_grid = xgb_param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    refit=True,\n",
    "    cv=fold_cross_validation,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "xgb_gridSearch.fit(X=X_train, y=y_train, **xgb_fit_params)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(f\"Training Time: {(end-start)/60:.2f} minutes\")\n",
    "\n",
    "pd.DataFrame(xgb_gridSearch.cv_results_)[[\n",
    "    'rank_test_score', 'param_xgb__booster', 'mean_test_score', \n",
    "    'std_test_score', 'mean_fit_time'\n",
    "]].sort_values(by='rank_test_score').set_index('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_model = xgb_gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(xgb_best_model, '../model/xgb_best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_all = data_train_origin.copy() # pd.read_csv(path_data_train, index_col = 'flightid')\n",
    "\n",
    "data_train_all = dropOutlier(\n",
    "    data_train_all, ['duration'],\n",
    "    lower_percentile=0.1,\n",
    "    upper_percentile=0.9\n",
    ")\n",
    "\n",
    "\n",
    "data_train_all['route'] = data_train_all['origem'] + '_' + data_train_all['destino']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_all = pd.read_csv(path_data_test, index_col = 'flightid', delimiter=';')\n",
    "\n",
    "data_test_all['route'] = data_test_all['origem'] + '_' + data_test_all['destino']\n",
    "\n",
    "_X_train_mxgb = dict()\n",
    "_X_valid_mxgb = dict()\n",
    "_X_test_mxgb = dict()\n",
    "y_train_mxgb = dict()\n",
    "y_valid_mxgb = dict()\n",
    "preprocessor_mxgb = dict()\n",
    "\n",
    "for route in data_train_all['route'].unique():\n",
    "        \n",
    "    transformer_mxgb = Pipeline(steps=[\n",
    "        # ('trans_stand', StandardScaler(with_mean=True)),\n",
    "        ('trans_norm', Normalizer()),\n",
    "        # ('trans_sparse_pca', SparsePCA(n_components=pca_n_components_mxgb)),\n",
    "        # ('trans_pca', PCA(n_components=pca_n_components_mxgb))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline\n",
    "    preprocessor_mxgb[route] = Pipeline(steps=[\n",
    "        ('imputer', imputer),\n",
    "        ('feature_engineer', feature_engineer),\n",
    "        ('remove_const_attr', RemoveConstantAttribute()),\n",
    "        ('transformer', transformer_mxgb)\n",
    "    ])\n",
    "        \n",
    "    ## Training and Validation Set\n",
    "    \n",
    "    data_train_mxgb = data_train_all[data_train_all['route'] == route]\n",
    "    \n",
    "    # Separate target from predictors\n",
    "    X_mxgb = data_train_mxgb.drop(['duration'], axis=1)\n",
    "    y_mxgb = data_train_mxgb.duration\n",
    "    \n",
    "    # Divide data into training and validation subsets\n",
    "    if len(X_mxgb) > 1:\n",
    "        X_train_full, X_valid_full, y_train_temp, y_valid_temp = train_test_split(X_mxgb, y_mxgb, train_size=0.8)\n",
    "    \n",
    "    else:\n",
    "        X_train_full = X_mxgb\n",
    "        X_valid_full = X_mxgb\n",
    "        y_train_temp = y_mxgb\n",
    "        y_valid_temp = y_mxgb\n",
    "     \n",
    "    # Keep selected columns only\n",
    "    X_train_mxgb = X_train_full[selected_cols].copy()\n",
    "    X_valid_mxgb = X_valid_full[selected_cols].copy()\n",
    "    X_test_mxgb = data_test_all[data_test_all['route'] == route][selected_cols].copy()\n",
    "    \n",
    "    if len(X_train_mxgb) == 0:\n",
    "        X_train_mxgb = X_valid_mxgb\n",
    "    \n",
    "    y_train_mxgb[route] = y_train_temp\n",
    "    y_valid_mxgb[route] = y_valid_temp\n",
    "    \n",
    "    \n",
    "    print(f\"{route}: train: {len(X_train_mxgb)} - valid: {len(X_valid_mxgb)} - test: {len(X_test_mxgb)}\")\n",
    "    \n",
    "    _X_train_mxgb[route] = preprocessor_mxgb[route].fit_transform(X_train_mxgb, y_train_mxgb[route])\n",
    "    _X_valid_mxgb[route] = preprocessor_mxgb[route].transform(X_valid_mxgb)\n",
    "    \n",
    "    if len(X_test_mxgb) > 0:\n",
    "        _X_test_mxgb[route] = preprocessor_mxgb[route].transform(X_test_mxgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X_train_mxgb = dict()\n",
    "_X_valid_mxgb = dict()\n",
    "_X_test_mxgb = dict()\n",
    "y_train_mxgb = dict()\n",
    "y_valid_mxgb = dict()\n",
    "preprocessor_mxgb = dict()\n",
    "\n",
    "for route in data_train_all['route'].unique():\n",
    "        \n",
    "    transformer_mxgb = Pipeline(steps=[\n",
    "        # ('trans_stand', StandardScaler(with_mean=True)),\n",
    "        ('trans_norm', Normalizer()),\n",
    "        # ('trans_sparse_pca', SparsePCA(n_components=pca_n_components_mxgb)),\n",
    "        # ('trans_pca', PCA(n_components=pca_n_components_mxgb))\n",
    "    ])\n",
    "    \n",
    "    # Pipeline\n",
    "    preprocessor_mxgb[route] = Pipeline(steps=[\n",
    "        ('imputer', imputer),\n",
    "        ('feature_engineer', feature_engineer),\n",
    "        ('remove_const_attr', RemoveConstantAttribute()),\n",
    "        ('transformer', transformer_mxgb)\n",
    "    ])\n",
    "        \n",
    "    ## Training and Validation Set\n",
    "    \n",
    "    data_train_mxgb = data_train_all[data_train_all['route'] == route]\n",
    "    \n",
    "    # Separate target from predictors\n",
    "    X_mxgb = data_train_mxgb.drop(['duration'], axis=1)\n",
    "    y_mxgb = data_train_mxgb.duration\n",
    "    \n",
    "    # Divide data into training and validation subsets\n",
    "    if len(X_mxgb) > 1:\n",
    "        X_train_full, X_valid_full, y_train_temp, y_valid_temp = train_test_split(X_mxgb, y_mxgb, train_size=0.8)\n",
    "    \n",
    "    else:\n",
    "        X_train_full = X_mxgb\n",
    "        X_valid_full = X_mxgb\n",
    "        y_train_temp = y_mxgb\n",
    "        y_valid_temp = y_mxgb\n",
    "     \n",
    "    # Keep selected columns only\n",
    "    X_train_mxgb = X_train_full[selected_cols].copy()\n",
    "    X_valid_mxgb = X_valid_full[selected_cols].copy()\n",
    "    X_test_mxgb = data_test_all[data_test_all['route'] == route][selected_cols].copy()\n",
    "    \n",
    "    if len(X_train_mxgb) == 0:\n",
    "        X_train_mxgb = X_valid_mxgb\n",
    "    \n",
    "    y_train_mxgb[route] = y_train_temp\n",
    "    y_valid_mxgb[route] = y_valid_temp\n",
    "    \n",
    "    \n",
    "    print(f\"{route}: train: {len(X_train_mxgb)} - valid: {len(X_valid_mxgb)} - test: {len(X_test_mxgb)}\")\n",
    "    \n",
    "    _X_train_mxgb[route] = preprocessor_mxgb[route].fit_transform(X_train_mxgb, y_train_mxgb[route])\n",
    "    _X_valid_mxgb[route] = preprocessor_mxgb[route].transform(X_valid_mxgb)\n",
    "    \n",
    "    if len(X_test_mxgb) > 0:\n",
    "        _X_test_mxgb[route] = preprocessor_mxgb[route].transform(X_test_mxgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxgb_model = dict()\n",
    "\n",
    "for route in data_train_all['route'].unique():  \n",
    "    \n",
    "    print(f\"Training route: {route}\")\n",
    "    \n",
    "    mxgb = XGBRegressor(\n",
    "        tree_method= 'hist',\n",
    "        device=\"cuda\",\n",
    "        objective='reg:squarederror',\n",
    "        eval_metric=mean_squared_error,\n",
    "        early_stopping_rounds=100,\n",
    "        verbosity = verbose\n",
    "    )  \n",
    "    \n",
    "    mxgb_regressor = Pipeline(\n",
    "        steps=[\n",
    "            ('mxgb', mxgb)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    mxgb_param_grid = {\n",
    "        'mxgb__booster': ['gbtree'], # 'gbtree', 'dart', 'gblinear'\n",
    "        'mxgb__n_estimators': [xgb_estimators], # normalmente entre 50 e 1000\n",
    "\n",
    "        ##### Parâmetros para Tree Booster (booster = gbtree) ##########\n",
    "\n",
    "        'mxgb__learning_rate': [0.1], # de 0 a 1\n",
    "        'mxgb__gamma': [0],\n",
    "        'mxgb__max_depth': [5],\n",
    "        'mxgb__min_child_weight': [5], # 1\n",
    "        'mxgb__max_delta_step': [0],\n",
    "        'mxgb__subsample': [1],\n",
    "        'mxgb__sampling_method': ['uniform'], # 'uniform', 'gradient_based', \n",
    "        'mxgb__colsample_bytree': [1], # de 0 a 1\n",
    "        'mxgb__colsample_bylevel': [1], # de 0 a 1\n",
    "        'mxgb__colsample_bynode': [1], # de 0 a 1\n",
    "        'mxgb__scale_pos_weight': [1],\n",
    "        'mxgb__grow_policy': ['depthwise'], # 'depthwise', 'lossguide'\n",
    "        'mxgb__max_leaves': [0],\n",
    "        'mxgb__max_bin': [256],\n",
    "        'mxgb__num_parallel_tree': [1],\n",
    "        'mxgb__refresh_leaf': [1],\n",
    "\n",
    "        #################################################################\n",
    "\n",
    "        # Parâmetros para Dart Booster (booster = dart) #################\n",
    "\n",
    "        'mxgb__sample_type': ['uniform'], # 'uniform', 'weighted'\n",
    "        'mxgb__normalize_type': ['tree'], # 'tree', 'forest'\n",
    "        'mxgb__rate_drop': [0], # de 0 a 1\n",
    "        'mxgb__one_drop': [0], \n",
    "        'mxgb__skip_drop': [0],\n",
    "\n",
    "        #################################################################\n",
    "\n",
    "        # Parâmetros para Linear Booster (booster = gblinear) ###########\n",
    "\n",
    "        'mxgb__feature_selector': ['cyclic'], # 'cyclic', 'shuffle', 'random', 'greedy', 'thrifty',  \n",
    "        'mxgb__top_k': [0]\n",
    "\n",
    "        #################################################################   \n",
    "    }\n",
    "    \n",
    "    mxgb_fit_params = {\n",
    "        'mxgb__eval_set': [(_X_valid_mxgb[route], y_valid_mxgb[route])]\n",
    "    }\n",
    "    \n",
    "    mxgb_gridSearch = GridSearchCV(\n",
    "        estimator = mxgb_regressor,\n",
    "        param_grid = mxgb_param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=1,\n",
    "        refit=True,\n",
    "        cv=5,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    mxgb_gridSearch.fit(X=_X_train_mxgb[route], y=y_train_mxgb[route], **mxgb_fit_params)\n",
    "    mxgb_model[route] = mxgb_gridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_ann = ann_best_model.predict(X_test, verbose=verbose)\n",
    "\n",
    "submission_ann = pd.DataFrame(y_predict_ann, index=X_test.index).reset_index()\n",
    "\n",
    "submission_ann.columns = ['ID', 'solution']\n",
    "submission_ann.to_csv(f\"../data/submission/idsc_submission_ann_{datetime.now().strftime('%d-%B-%Ih%Mmin')}.csv\", index=False)\n",
    "\n",
    "submission_ann.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_xgb = xgb_best_model.predict(X_test)\n",
    "\n",
    "submission_xgb = pd.DataFrame(y_predict_xgb, index=X_test.index).reset_index()\n",
    "\n",
    "submission_xgb.columns = ['ID', 'solution']\n",
    "submission_xgb.to_csv(f\"../data/submission/idsc_submission_xgb_{datetime.now().strftime('%d-%B-%Ih%Mmin')}.csv\", index=False)\n",
    "\n",
    "submission_xgb.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_all.loc[:, 'solution'] = None\n",
    "\n",
    "for route in data_test_all['route'].unique():\n",
    "    try:\n",
    "        prediction = mxgb_model[route].predict(_X_test_mxgb[route])\n",
    "    except:\n",
    "        prediction = 0\n",
    "\n",
    "    index = data_test_all[data_test_all['route'] == route].index\n",
    "    data_test_all.loc[index,'solution'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_mxgb = data_test_all.reset_index()[['flightid', 'solution']]\n",
    "\n",
    "submission_mxgb.columns = [['ID', 'solution']]\n",
    "submission_mxgb.to_csv(f\"../data/submission/idsc_submission_mxgb_{datetime.now().strftime('%d-%B-%Ih%Mmin')}.csv\", index=False)\n",
    "\n",
    "\n",
    "submission_mxgb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
